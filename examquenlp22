Список вопросов к экзамену (осень 2022) -- курс Арефьева Николая Викторовича

 1 Text classification. Bag of words / bag of n-grams representations. Zipf’s law. 
 2 Naive Bayes classifier: multinomial and bernoulli models. Laplace (alpha) smoothing.
 3 Linear regression. Perceptron.
 4 Logistic regression. Cross entropy loss. Multiclass/multilabel classification.
 5 Gradient descent. Full-batch versus stochastic gradient descent. Initialization and update rule for LR/FFNN.
 6 Feed-forward (fully connected) NN. Activation functions. NN as universal function approximator. Initialization and SGD update rule.
 7 Backpropagation. Derivation of gradients and weights update rule for binary/multiclass FFNN classifier with 1 hidden layer / K hidden layers.
 8 Training NNs with SGD: problems and solutions. Exponentially weighted average. Momentum, Adagrad, RMSProp, Adam.
 9 Classifier quality evaluation, surrogate loss functions. Expected versus empirical risk.
 10 Capacity / effective capacity of ML algorithm. Regularization. Generalization error decomposition. Bias/Variance tradeoff.
 11 Hyperparameter selection. Train/dev/test split. Cross Validation.
 12 Language models. Perplexity. Recurrent language model. 

 13 Vanishing/exploding gradient problem. LSTM. Gradient clipping.



Примеры дополнительных вопросов (validation set)

Некоторые примеры дополнительных вопросов. Приводится лишь небольшая часть вопросов - рекомендуем использовать их в качестве валидационной выборки для самопроверки, чтобы избежать переобучения.
0) Определение градиента функции. Геометрический смысл градиента. Найти градиент функции x**2 + 2 * y**2, изобразить линии уровня этой функции и траекторию градиентного спуска.
1) Почему нельзя инициализировать веса FFNN единицами? Что будет, если инициализировать их слишком большими / маленькими числами и почему? (требуется выписать формулы обновления весов и показать в них соответствующие проблемы)
2) Почему SGD плохо работает в ситуации, когда разные признаки имеют сильно отличающийся диапазон значений?
3) Из каких соображений выбираются распределения для инициализации весов сетей? (вывести параметры распределений)
4) Почему линейная регрессия плохо подходит для классификации?
5) Почему для логистической регрессии используется кросс-энтропия в качестве функции потерь?
6) Проблема затухания градиента в рекуррентных сетях - градиент по каким параметрам затухает, почему и при каких условиях, к каким именно проблемам это приводит?
7) Рекуррентная языковая модель: выписать формулы прямого прохода для входа "I love", указать размерности всех векторов и матриц в формулах, пояснить стоящую за формулами интуицию. 
8) В чем отличия в формулах прямого прохода рекуррентных ячеек LSTM и Vanilla RNN? Как это помогает уменьшить проблему затухания градиента? Почему это не решает проблему затухания градиента полностью?



Запись лекций: https://www.youtube.com/playlist?list=PLxEQA5k0LBt6PAORtrBl9EzXeY8fguS2M










Вопросы к зачету / зкзамену

 1 Document embeddings: PV-DBOW, PV-DM, DV-ngram models.
 2 Document embeddings: Cosine DV-ngram. Combining document embeddings and bag-of-ngrams.
 3 Regularization of recurrent language models. Early stopping, L2-regularization / weight decay, MaxNorm. Dropout.
 4 Dropout in recurrent language models. Zaremba LM. Recurrent dropout. Embedding dropout. Weight dropout / dropconnect.
 5 Tied input/output embeddings. 
 6 LM datasets. Neural cache.

 7 AWD-LSTM model: regularization and optimization. Ablation analysis.
 8 LSTM and QRNN recurrent cells. AWD-QRNN.
 9 Sampled softmax. Adaptive softmax.

 10 Basic attention in neural machine translation. 
 11 Attention for Machine Translation with RNNs (Bahdanau model). 
 12 Attention blocks in Transformer encoder and decoder. Scaled dot-product attention, multihead attention, masked self-attention. Positional encoding.
 13 Transformer for Machine Translation. Architecture of a Transformer block, encoder, decoder. Normalization and residual connections.
 14 Transformer for Machine Translation. Embeddings, regularization, optimization

 15 BERT architecture, pre-training, fine-tuning.
 16 RoBERTa and XLM-R: architecture, pre-training, fine-tuning. Differences with BERT.
